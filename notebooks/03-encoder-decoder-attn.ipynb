{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The annotated encoder-decoer with attention\n",
    "\n",
    "Based on [this blog](https://bastings.github.io/annotated_encoder_decoder/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an input sequence $X=(x_1, ..., x_M)$ and a target sequence $Y=(y_1, ..., y_N)$. We will model the probability $p(Y|X)$ directly with a neural network: an encoder-decoder.\n",
    "\n",
    "<img src=\"../figures/enc.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder reads in the source sentence (bottom of above figure) and produces a sequence of hidden states $\\mathbf{h_1,...,h_M}$ for each input word. These hidden states should capture the meaning of a word **in its context** given the sentence. Use a Bi-GRU as the encoder.\n",
    "\n",
    "We first embed the source words: each source word's embedding is denoted as a vector $\\mathbf{x}_i$. Using an embedding allows us to exploit the fact that certain words are semantically similar, and should therefore be processed in a similar way by having close embedding vectors.\n",
    "\n",
    "We obtain the hidden states $\\mathbf{h_1,...,h_M}$ from the recursive formula\n",
    "$$\\mathbf{h}_j = GRU(\\mathbf{x}_j, \\mathbf{h}_{j-1})$$\n",
    "for a forward GRU. A backward GRU reads from right-to-left instead of left-to-right. We therefore obtain two hidden state vectors for each word. Concatenating the two hidden states together allows us to obtain a local context for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder is also a GRU with hidden state $\\mathbf{s}_i$. It follows a similar formula to the encoder, but takes one extra input $\\mathbf{c}_i$ (shown in yellow) called the **context**\n",
    "$$\\mathbf{s}_i = f(\\mathbf{s}_{i-1}, \\mathbf{y}_{i-1}, \\mathbf{c}_{i})$$\n",
    "where $\\mathbf{y}_{i-1}$ is the embedding for the previously generated target word.\n",
    "\n",
    "At each time step, an **attention mechanism** dynamically selects the part of the source sentence that is most relevant for predicting the current target word. It does so by comparing the last decoder state  with each source hidden state (**TODO: FILL IN MATHS**)\n",
    "\n",
    "After computing the decoder state $\\mathbf{s}_i$, a non-linear function $g$ (softmax) gives us the probability of the target word $y_i$ for this time step:\n",
    "$$p(y_i|y_{<i},x_1^M) = g(\\mathbf{s}_{i-1}, \\mathbf{y}_{i-1}, \\mathbf{c}_{i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE=torch.device('cuda:0' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed);\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
